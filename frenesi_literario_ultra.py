#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
FREN√äSI LITER√ÅRIO‚Ñ¢ ULTRA

Analisador estil√≠stico para texto de fic√ß√£o extra√≠do de PDF.

Funcionalidades:
- Extrai texto de PDF
- Divide em cap√≠tulos (por padr√£o, cabe√ßalhos "CAP√çTULO X")
- Calcula m√©tricas globais e por cap√≠tulo:
    - total de palavras / frases
    - tamanho m√©dio das frases
    - TTR, MTLD, HDD (LexicalRichness)
    - densidade lexical por blocos
    - √≠ndice de repeti√ß√£o
    - contagem de adv√©rbios em -mente
    - verbos fracos e fortes
    - top 20 palavras (sem stopwords)
    - Nota FREN√äSI‚Ñ¢ (0‚Äì100) e diagn√≥stico

- Gera:
    - relat√≥rio em texto (.txt)
    - arquivo de texto com destaques:
        [FRACO: verbo], [FORTE: verbo], [ADV: palavra]

Requisitos:
    pip install pdfminer.six spacy lexicalrichness
    python -m spacy download pt_core_news_sm
"""

import os
import re
import statistics
from collections import Counter

from pdfminer.high_level import extract_text
from lexicalrichness import LexicalRichness
import spacy
from spacy.lang.pt.stop_words import STOP_WORDS
import statistics

# Carrega modelo do spaCy
nlp = spacy.load("pt_core_news_sm")

# Stopwords em portugu√™s
STOPWORDS = set(STOP_WORDS)

# Verbos fracos (ajust√°vel)
VERBOS_FRACOS = {
    # Existenciais / vazios
    "ser", "estar", "ter", "haver", "existir", "acontecer", "ocorrer",

    # Liga√ß√£o
    "ficar", "parecer", "continuar", "permanecer", "tornar-se",
    "manter-se", "virar", "voltar-se",

    # Auxiliares
    "poder", "dever", "precisar", "costumar",
    "querer", "tentar", "conseguir", "deixar",

    # Cognitivos gen√©ricos
    "achar", "pensar", "imaginar", "acreditar",
    "notar", "perceber", "sentir", "ver", "saber",

    # Gen√©ricos universais
    "fazer", "colocar", "passar", "botar", "pegar", "voltar",

    # Conversacionais fracos
    "dizer", "falar", "comentar", "contar",
    "perguntar", "responder",

    # Descritivos vazios
    "aparecer", "surgir", "ir", "vir"
}

# Verbos fortes (ajust√°vel)
VERBOS_FORTES = {
    "avan√ßar", "atirar", "erguer", "arremessar", "agarrar", "arrastar",
    "golpear", "correr", "disparar", "desabar", "investir", "lan√ßar",
    "rasgar", "puxar", "empurrar", "invadir", "mergulhar", "tombar",
    "derrubar", "cravar", "pular", "arrebentar", "esmagar", "socar",

    "encarar", "espiar", "detectar", "farejar", "examinar", "vasculhar",
    "observar", "apalpar", "reconhecer", "fitar",

    "amea√ßar", "provocar", "confrontar", "acusar", "pressionar",
    "encurralar", "dominar", "intimidar", "desafiar", "subjugar",
    "interrogar",

    "latejar", "ressoar", "treme√ßar", "contrair", "vibrar", "pulsar",
    "engolir", "sufocar",

    "romper", "quebrar", "transformar", "distorcer", "moldar", "curvar",
    "gelar", "aquecer", "converter", "estilha√ßar", "enrijecer",

    "decidir", "afirmar", "insistir", "negar", "jurar", "ordenar",
    "convocar", "implorar",

    "ecoar", "silvar", "estalhar", "vazar", "chorar", "zumbir",
    "chispar", "estourar",

    "agredir", "acariciar", "consolar", "afastar", "aproximar",
    "abrigar", "socorrer", "amparar", "abra√ßar", "segurar",

    "rosnar", "grunhir", "sussurrar", "berrar", "murmurar", "bufar",
    "retrucar", "gritar", "soltar", "vomitar"
}

def marcar_monotonia_texto(lista):
    saida = []
    for item in lista:
        if item["nivel"] == "Mon√≥tono":
            saida.append(f"[MONOTONO] {item['paragrafo']}")
        else:
            saida.append(item["paragrafo"])
    return "\n\n".join(saida)

def salvar_relatorio_monotonia(lista, caminho):
    with open(caminho, "w", encoding="utf-8") as f:
        f.write("RELAT√ìRIO DE MONOTONIA POR PAR√ÅGRAFO\n")
        f.write("=====================================\n\n")

        for item in lista:
            f.write(f"Par√°grafo {item['numero']}\n")
            f.write(f"Classifica√ß√£o: {item['nivel']}\n")
            f.write(f"Tamanhos das frases: {item['frases']}\n")
            f.write(f"√çndice de Ritmo (IR): {item['ir']:.3f}\n")
            f.write(f"Altern√¢ncia (SW): {item['sw']}\n")
            f.write("-" * 50 + "\n")


def analisar_monotonia_por_paragrafo(texto: str):
    paragrafos = [p.strip() for p in texto.split("\n") if p.strip()]

    resultados = []
    for i, p in enumerate(paragrafos, start=1):
        m = medir_monotonia(p)
        resultados.append({
            "numero": i,
            "paragrafo": p,
            "frases": m["frases"],
            "ir": m["ir"],
            "sw": m["sw"],
            "nivel": m["nivel"]
        })

    return resultados


def medir_monotonia(paragrafo: str):
    doc = nlp(paragrafo)

    # tamanhos das frases
    tamanhos = [len(sent.text.split()) for sent in doc.sents]

    if len(tamanhos) == 0:
        return {"frases": [], "ir": 0, "sw": 0, "nivel": "Indeterminado"}

    # √≠ndice de ritmo (desvio / m√©dia)
    media = statistics.mean(tamanhos)
    desvio = statistics.pstdev(tamanhos)
    ir = desvio / media if media > 0 else 0

    # categorizar frases
    def cat(t):
        if t < 10:
            return "C"   # curta
        elif t <= 20:
            return "M"   # m√©dia
        else:
            return "L"   # longa

    cats = [cat(t) for t in tamanhos]

    # switch index
    sw = sum(1 for i in range(1, len(cats)) if cats[i] != cats[i-1])

    # classifica√ß√£o final
    if ir < 0.15 and sw <= 1:
        nivel = "Mon√≥tono"
    elif ir < 0.30:
        nivel = "Ritmo moderado"
    elif ir < 0.55:
        nivel = "Variado"
    else:
        nivel = "Irregular / Ca√≥tico"

    return {
        "frases": tamanhos,
        "ir": ir,
        "sw": sw,
        "nivel": nivel
    }


def frases_numeradas_somente_marcadas(texto: str):
    """
    Divide o texto em frases e mant√©m apenas as que t√™m marca√ß√µes
    [FRACO:], [FORTE:], [ADV:].
    Numeradas.
    """
    # Divide sem usar spaCy para preservar marca√ß√µes
    frases = re.split(r'(?<=[.!?])\s+', texto)

    linhas = []
    n = 1

    for f in frases:
        frase = f.strip()

        # mant√©m s√≥ as que t√™m alguma etiqueta
        if ("[FRACO:" in frase or
            "[FORTE:" in frase or
            "[ADV:" in frase):

            linhas.append(f"{n}. {frase}")
            n += 1

    return "\n".join(linhas)

def frases_numeradas_preservando(texto: str):
    """
    Divide o texto em frases mantendo qualquer marca√ß√£o
    (como [FRACO:], [FORTE:]) intacta.
    N√£o usa spaCy, usa regex.
    """
    # Divide ap√≥s ., ?, ou ! seguidos de espa√ßo
    frases = re.split(r'(?<=[.!?])\s+', texto)

    linhas = []
    n = 1
    for f in frases:
        frase = f.strip()
        if frase:
            linhas.append(f"{n}. {frase}")
            n += 1

    return "\n".join(linhas)


def remover_dialogos(texto: str) -> str:
    """
    Remove di√°logos entre aspas e linhas iniciadas por travess√£o.
    Mant√©m apenas a narrativa.
    """

    # Remove trechos entre aspas ‚Äú ‚Äù
    texto = re.sub(r"[\"‚Äú‚Äù](.*?)[\"‚Äú‚Äù]", "", texto)

    # Remove linhas que come√ßam com travess√£o ‚Äî (casos cl√°ssicos de di√°logo)
    linhas = texto.split("\n")
    linhas_filtradas = [l for l in linhas if not l.strip().startswith("‚Äî")]

    return "\n".join(linhas_filtradas)

def extrair_texto_pdf(caminho_pdf: str) -> str:
    """Extrai texto bruto de um arquivo PDF."""
    return extract_text(caminho_pdf)


def limpar(texto: str) -> str:
    """Limpa quebras de linha excessivas e normaliza espa√ßos."""
    texto = texto.replace("\r", " ")
    texto = texto.replace("\n", " ")
    texto = re.sub(r"\s+", " ", texto)
    return texto.strip()


def segmentar_frases(texto: str):
    """Segmenta texto em frases usando spaCy."""
    doc = nlp(texto)
    return [sent.text for sent in doc.sents if sent.text.strip()]


def split_capitulos(texto_bruto: str):
    """
    Divide o texto em cap√≠tulos com base em cabe√ßalhos do tipo:
    'CAP√çTULO 1', 'Cap√≠tulo I', etc.

    Se n√£o encontrar nenhum padr√£o, retorna um cap√≠tulo √∫nico.
    """
    pattern = r'(?im)^(cap[√≠i]tulo\s+[^\n]+)'
    partes = re.split(pattern, texto_bruto)

    capitulos = []

    if len(partes) <= 1:
        # N√£o encontrou nenhum cabe√ßalho de cap√≠tulo
        capitulos.append({
            "titulo": "Texto completo",
            "texto": texto_bruto.strip()
        })
        return capitulos

    # partes = [antes, titulo1, texto1, titulo2, texto2, ...]
    # ignorar "antes" se for s√≥ lixo
    prefixo = partes[0].strip()
    idx = 1

    if prefixo:
        capitulos.append({
            "titulo": "Pr√©-texto",
            "texto": prefixo
        })

    while idx < len(partes) - 1:
        titulo = partes[idx].strip()
        corpo = partes[idx + 1].strip()
        capitulos.append({
            "titulo": titulo,
            "texto": corpo
        })
        idx += 2

    return capitulos


def densidade_lexical(texto: str, janela: int = 100) -> float:
    """Calcula TTR m√©dio em blocos de tamanho fixo."""
    palavras = re.findall(r"\w+", texto.lower())
    if not palavras:
        return 0.0
    blocos = [palavras[i:i + janela] for i in range(0, len(palavras), janela)]
    ttrs = [len(set(b)) / len(b) for b in blocos if len(b) == janela]
    return sum(ttrs) / len(ttrs) if ttrs else 0.0


def indice_repeticao(texto: str) -> float:
    """Propor√ß√£o de repeti√ß√£o lexical: 1 - (types/tokens)."""
    palavras = re.findall(r"\w+", texto.lower())
    if not palavras:
        return 0.0
    return 1 - (len(set(palavras)) / len(palavras))


def frenesi_score(stats: dict) -> float:
    """Calcula uma Nota FREN√äSI‚Ñ¢ de 0 a 100 a partir das m√©tricas."""
    score = 50.0

    # TTR
    ttr = stats.get("ttr", 0)
    if 0.20 <= ttr <= 0.40:
        score += 10
    elif ttr < 0.18 or ttr > 0.45:
        score -= 5

    # MTLD
    mtld = stats.get("mtld", 0)
    if mtld >= 50:
        score += 10
    elif mtld < 30:
        score -= 5

    # Verbos fortes vs fracos
    vf = stats.get("verbos_fortes", 0)
    vfr = stats.get("verbos_fracos", 0)
    total_v = vf + vfr
    ratio_fortes = vf / total_v if total_v > 0 else 0
    stats["ratio_verbos_fortes"] = ratio_fortes

    if ratio_fortes >= 0.25:
        score += 10
    elif ratio_fortes < 0.10 and total_v > 0:
        score -= 10

    # Adv√©rbios em -mente
    adv_mente = stats.get("adv_mente", 0)
    total_palavras = max(stats.get("total_palavras", 0), 1)
    adv_pct = adv_mente / total_palavras
    if adv_pct > 0.03:
        score -= 5

    # √çndice de repeti√ß√£o
    rep = stats.get("indice_repeticao", 0)
    if rep < 0.78:
        score += 5
    elif rep > 0.88:
        score -= 5

    # Tamanho m√©dio das frases
    tmf = stats.get("tamanho_medio_frase", 0)
    if 12 <= tmf <= 24:
        score += 5
    elif tmf > 32 or tmf < 8:
        score -= 5

    # Clamp 0‚Äì100
    score = max(0.0, min(100.0, score))
    return score


def diagnostico(stats: dict):
    """Gera diagn√≥stico textual a partir das m√©tricas."""
    avisos = []

    ttr = stats.get("ttr", 0)
    if ttr < 0.18:
        avisos.append("Vocabul√°rio repetitivo (TTR baixo). Refor√ßar sin√¥nimos.")
    elif ttr > 0.45:
        avisos.append("Vocabul√°rio muito alto (TTR alto). Cuidado para n√£o soar artificial.")

    if stats.get("adv_mente", 0) > 25:
        avisos.append("Uso elevado de adv√©rbios em -mente. Risco de prosa a√ßucarada.")

    vfr = stats.get("verbos_fracos", 0)
    total_palavras = max(stats.get("total_palavras", 0), 1)
    if vfr > total_palavras * 0.06:
        avisos.append("Alta incid√™ncia de verbos fracos. Narrativa pode estar passiva.")

    tmf = stats.get("tamanho_medio_frase", 0)
    if tmf > 28:
        avisos.append("Frases muito longas em m√©dia. Considerar cortes e quebras.")
    elif tmf < 10 and tmf > 0:
        avisos.append("Frases curtas demais em m√©dia. Risco de estilo telegr√°fico.")

    rep = stats.get("indice_repeticao", 0)
    if rep > 0.88:
        avisos.append("Repeti√ß√£o lexical elevada. Pode haver barriga narrativa.")
    elif rep < 0.75 and stats.get("total_palavras", 0) > 500:
        avisos.append("Baixa repeti√ß√£o lexical para texto longo. Ver se n√£o h√° excesso de varia√ß√£o gratuita.")

    if not avisos:
        avisos.append("Estilisticamente saud√°vel. Nenhum problema evidente nas m√©tricas gerais.")

    return avisos

def contar_verbos_fracos_fortes(doc):
    fracos = 0
    fortes = 0

    for token in doc:
        if token.pos_ == "VERB":
            lemma = token.lemma_.lower()

            if lemma in VERBOS_FRACOS:
                fracos += 1
            elif lemma in VERBOS_FORTES:
                fortes += 1

    return fracos, fortes


def examinar_texto(texto: str) -> dict:
    """Executa toda a an√°lise em um trecho de texto."""
    texto_limpo = limpar(texto)

    palavras = re.findall(r"\w+", texto_limpo.lower())
    total_palavras = len(palavras)

    # Frases
    frases = segmentar_frases(texto_limpo)
    tamanhos_frase = [len(f.split()) for f in frases] if frases else []

    # An√°lise spaCy
    doc = nlp(texto_limpo)

    # Vocabul√°rio
    palavras_filtradas = [p for p in palavras if p not in STOPWORDS]
    contador = Counter(palavras_filtradas)

    # LexicalRichness
    if texto_limpo.strip():
        lex = LexicalRichness(texto_limpo)
        ttr = lex.ttr
        mtld = lex.mtld()
        hdd = lex.hdd()
    else:
        ttr = mtld = hdd = 0.0

    # Adv√©rbios em -mente (de verdade, n√£o qualquer coisa que termina em "mente")
    adv_mente_tokens = [
        token for token in doc
        if token.pos_ == "ADV" and token.text.lower().endswith("mente")
    ]

    # Usa o spaCy para analisar o texto inteiro como documento lingu√≠stico
    doc = nlp(texto_limpo)

    # Verbos fracos e fortes: agora detectados por LEMMA, incluindo flex√µes
    vf, vfo = contar_verbos_fracos_fortes(doc)


    stats = {
        "total_palavras": total_palavras,
        "total_frases": len(frases),
        "tamanho_medio_frase": statistics.mean(tamanhos_frase) if tamanhos_frase else 0.0,
        "tamanho_max_frase": max(tamanhos_frase) if tamanhos_frase else 0,
        "tamanho_min_frase": min(tamanhos_frase) if tamanhos_frase else 0,
        "top_20_palavras": contador.most_common(20),
        "adv_mente": len(adv_mente_tokens),
        "verbos_fracos": vf,
        "verbos_fortes": vfo,
        "ttr": ttr,
        "mtld": mtld,
        "hdd": hdd,
        "densidade_lexical": densidade_lexical(texto_limpo),
        "indice_repeticao": indice_repeticao(texto_limpo),
    }

    # Nota FREN√äSI‚Ñ¢
    stats["nota_frenesi"] = frenesi_score(stats)

    return stats


def gerar_highlight(texto: str) -> str:
    """
    Gera texto com destaques:
    - [FRACO:tok]
    - [FORTE:tok]
    - [ADV:tok]    (somente adv√©rbios reais terminados em -mente)
    """
    doc = nlp(texto)  # spaCy para POS + lemma
    resultado = []

    for token in doc:
        tok = token.text
        base = tok.lower()

        # Verbo fraco pelo lemma
        if token.pos_ == "VERB" and token.lemma_.lower() in VERBOS_FRACOS:
            resultado.append(f"[FRACO:{tok}]")

        # Verbo forte pelo lemma
        elif token.pos_ == "VERB" and token.lemma_.lower() in VERBOS_FORTES:
            resultado.append(f"[FORTE:{tok}]")

        # Adv√©rbio REAL em -mente
        elif token.pos_ == "ADV" and base.endswith("mente"):
            resultado.append(f"[ADV:{tok}]")

        # Caso contr√°rio, reproduz o token original
        else:
            resultado.append(tok)

        # Adiciona o whitespace original (spaCy mant√©m)
        resultado.append(token.whitespace_)

    return "".join(resultado)


def salvar_relatorio(global_stats: dict, capitulos_stats: list, caminho_saida: str):
    """Salva o relat√≥rio completo em formato Markdown."""
    with open(caminho_saida, "w", encoding="utf-8") as f:

        # T√≠tulo
        f.write("# FREN√äSI LITER√ÅRIO‚Ñ¢ ULTRA ‚Äì Relat√≥rio Completo\n\n")

        # =============================
        # M√âTRICAS GLOBAIS
        # =============================
        f.write("## üìä M√©tricas Globais\n")
        f.write(f"- **Total de palavras:** {global_stats['total_palavras']}\n")
        f.write(f"- **Total de frases:** {global_stats['total_frases']}\n")
        f.write(f"- **Tamanho m√©dio das frases:** {global_stats['tamanho_medio_frase']:.2f}\n")
        f.write(f"- **Frase mais longa:** {global_stats['tamanho_max_frase']} palavras\n")
        f.write(f"- **Frase mais curta:** {global_stats['tamanho_min_frase']} palavras\n\n")

        # =============================
        # VOCABUL√ÅRIO
        # =============================
        f.write("## üß† Vocabul√°rio\n")
        f.write(f"- **TTR:** {global_stats['ttr']:.3f}\n")
        f.write(f"- **MTLD:** {global_stats['mtld']:.3f}\n")
        f.write(f"- **HDD:** {global_stats['hdd']:.3f}\n")
        f.write(f"- **Densidade lexical (por 100 palavras):** {global_stats['densidade_lexical']:.3f}\n")
        f.write(f"- **√çndice de repeti√ß√£o:** {global_stats['indice_repeticao']:.3f}\n\n")

        # =============================
        # VERBOS / ADV√âRBIOS
        # =============================
        f.write("## üîß Verbos e Adv√©rbios\n")
        f.write(f"- **Verbos fracos:** {global_stats['verbos_fracos']}\n")
        f.write(f"- **Verbos fortes:** {global_stats['verbos_fortes']}\n")

        total_v = global_stats["verbos_fracos"] + global_stats["verbos_fortes"]
        ratio_f = (global_stats["verbos_fortes"] / total_v) if total_v > 0 else 0.0

        f.write(f"- **Propor√ß√£o de verbos fortes:** {ratio_f:.3f}\n")
        f.write(f"- **Adv√©rbios terminados em -mente:** {global_stats['adv_mente']}\n\n")

        # =============================
        # TOP PALAVRAS
        # =============================
        f.write("## üîù Top 20 Palavras (sem stopwords)\n")
        f.write("| Palavra | Frequ√™ncia |\n")
        f.write("|---------|------------|\n")
        for palavra, freq in global_stats["top_20_palavras"]:
            f.write(f"| {palavra} | {freq} |\n")
        f.write("\n")

        # =============================
        # NOTA GLOBAL
        # =============================
        f.write("## üéØ Nota FREN√äSI‚Ñ¢ Global\n")
        f.write(f"**{global_stats['nota_frenesi']:.1f} / 100**\n\n")

        # =============================
        # DIAGN√ìSTICO
        # =============================
        f.write("## ü©∫ Diagn√≥stico Global\n")
        for aviso in diagnostico(global_stats):
            f.write(f"- {aviso}\n")
        f.write("\n")

        # =============================
        # AN√ÅLISE POR CAP√çTULO
        # =============================
        f.write("---\n")
        f.write("# üìö An√°lise por Cap√≠tulo\n\n")

        for cap in capitulos_stats:
            titulo = cap["titulo"]
            stats = cap["stats"]

            f.write(f"## {titulo}\n\n")

            # Tabela com m√©tricas b√°sicas
            f.write("| M√©trica | Valor |\n")
            f.write("|---------|-------|\n")
            f.write(f"| Palavras | {stats['total_palavras']} |\n")
            f.write(f"| Frases | {stats['total_frases']} |\n")
            f.write(f"| Tamanho m√©dio das frases | {stats['tamanho_medio_frase']:.2f} |\n")
            f.write(f"| Verbos fracos | {stats['verbos_fracos']} |\n")
            f.write(f"| Verbos fortes | {stats['verbos_fortes']} |\n")

            total_vc = stats["verbos_fracos"] + stats["verbos_fortes"]
            ratio_fc = stats["verbos_fortes"] / total_vc if total_vc > 0 else 0.0

            f.write(f"| Propor√ß√£o de verbos fortes | {ratio_fc:.3f} |\n")
            f.write(f"| TTR | {stats['ttr']:.3f} |\n")
            f.write(f"| MTLD | {stats['mtld']:.3f} |\n")
            f.write(f"| HDD | {stats['hdd']:.3f} |\n")
            f.write(f"| Nota FREN√äSI‚Ñ¢ | {stats['nota_frenesi']:.1f} |\n")
            f.write("\n")

            # Diagn√≥stico do cap√≠tulo
            f.write("### Diagn√≥stico\n")
            for aviso in diagnostico(stats):
                f.write(f"- {aviso}\n")
            f.write("\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="FREN√äSI LITER√ÅRIO‚Ñ¢ ULTRA ‚Äì Analisador de PDF")
    parser.add_argument("pdf", help="Arquivo PDF de entrada")
    parser.add_argument("saida", help="Arquivo .txt de sa√≠da (relat√≥rio)")
    args = parser.parse_args()

    print("Extraindo texto do PDF...")
    texto_bruto = extrair_texto_pdf(args.pdf)

    print("Removendo di√°logos...")
    texto_bruto = remover_dialogos(texto_bruto)

    print("Dividindo em cap√≠tulos...")
    capitulos = split_capitulos(texto_bruto)

    # Texto global (limpo) para an√°lise geral
    print("Preparando texto global...")
    texto_global = " ".join(limpar(c["texto"]) for c in capitulos)

    print("Analisando texto global...")
    global_stats = examinar_texto(texto_global)

    print("Analisando cap√≠tulos...")
    capitulos_stats = []
    for cap in capitulos:
        stats_cap = examinar_texto(cap["texto"])
        capitulos_stats.append({
            "titulo": cap["titulo"],
            "stats": stats_cap
        })

    print("Gerando relat√≥rio...")
    salvar_relatorio(global_stats, capitulos_stats, args.saida)

    # Gera texto com destaques
    base, ext = os.path.splitext(args.saida)
    destaque_path = base + "_highlight.txt"
    print("Gerando texto com destaques...")
    texto_destacado = gerar_highlight(texto_global)
    #texto_destacado = frases_numeradas_preservando(texto_destacado)
    texto_destacado = frases_numeradas_somente_marcadas(texto_destacado)


    with open(destaque_path, "w", encoding="utf-8") as fh:
        fh.write(texto_destacado)

    print(f"Relat√≥rio salvo em: {args.saida}")
    print(f"Texto destacado salvo em: {destaque_path}")

    print("Analisando monotonia por par√°grafo...")
    monotonia = analisar_monotonia_por_paragrafo(texto_global)

    rel_monotonia_path = base + "_monotonia_paragrafos.txt"
    salvar_relatorio_monotonia(monotonia, rel_monotonia_path)
    print("Relat√≥rio de monotonia salvo em:", rel_monotonia_path)


if __name__ == "__main__":
    main()
